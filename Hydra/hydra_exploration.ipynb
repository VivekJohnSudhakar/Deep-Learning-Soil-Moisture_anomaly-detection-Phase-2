{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fbb6d4-8de7-4445-b54d-9f67e77f15a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de39d67-eac6-472c-861a-9dee36f68ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c81664-fbd5-4691-a098-468084e96668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beaec2c-6061-4d8a-9f39-f38e8f84424d",
   "metadata": {},
   "source": [
    "### Get proportion of Nones in Variable col for each station\n",
    "The Variable column contains the name of the sensor within station. \n",
    "Each station contains 15 sensors, split across 5 depths and 3 holes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36030012-cc4d-4eff-8d8f-eb69efb67758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "parquet_file = pq.ParquetFile('s3://uscrn-soilmoisture-ml/shiva/hydra/parquet/my_parquet.parquet')\n",
    "tab = parquet_file.read(columns=['station_ID','Variable']).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc72b22d-b142-41a5-bfab-ebf4c3b2edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all varaibles for a station\n",
    "variable_df = tab.groupby(['station_ID'])['Variable'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dd6dca-2e4e-4494-af49-06e39d8a434f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = tab.groupby('station_ID').agg({'Variable': lambda x: x.isnull().mean()})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c52cfaa-ec7e-4cbe-b7c5-b6da4bb18e8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Stations with no Variable data \n",
    "station_no_variable = list(res[res.Variable==1.0].index)\n",
    "print(station_no_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c854718-0f8f-4d1c-b59b-b2a62a739f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Station with all variable data\n",
    "station_all_variable = list(res[res.Variable==0.0].index)\n",
    "print(station_all_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251e51c2-bb2a-4d5a-846f-b4ca78cdcf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_major_variable = list(res[res.Variable<0.1].index)\n",
    "print(len(station_major_variable))\n",
    "print(station_major_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b9d5f1-1823-49a1-9b90-e7e6eab13ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All unique values in Variable column\n",
    "tab['Variable'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72099ba8-a6a3-468c-afa4-ba9d18e8869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tab['station_ID'].unique()))\n",
    "print(tab['station_ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b44b8ec-fba3-4354-90bc-aa71cf460209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STations\n",
    "\n",
    "station_ids = \"\"\"1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014\n",
    " 1015 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029\n",
    " 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043\n",
    " 1045 1046 1047 1049 1050 1066 1067 1068 1085 1105 1106 1107 1108 1109\n",
    " 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123\n",
    " 1124 1125 1128 1129 1130 1138 1139 1140 1141 1142 1143 1144 1145 1166\n",
    " 1186 1187 1231 1232 1233 1234 1235 1236 1266 1267 1268 1269 1296 1297\n",
    " 1306 1307 1326 1327 1328 1329 1346 1347 1366 1386 1387 1406 1407 1426\n",
    " 1446 1447 1467 1468 1487 1507 1508 1509 1510 1528 1529 1530 1531 1532\n",
    " 1550 1551 1552 1570 1590 1610 1611 1612 1613 1630 1631 1632 1650 1651\n",
    " 1652 1653 1654 1655 1656 1657 1658 1660 1661 1670 1671 1691 1692 1694\n",
    " 1695 1696 1697 1698 1701 1702 1703 1704 1705 1706 1707 1708 1710 1711\n",
    " 1712 1713 1714 1715 1716 1717 1721 1722 1726 1727 1728 1729 1730 1731\n",
    " 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746\n",
    " 1747 1748 1749 1750 1753 1754 1755 1756 1771 1772 1773 1774 1775 1776\n",
    " 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790\n",
    " 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801\"\"\".split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab8b643-bc54-45dc-8492-3db956efe16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_ids = list(map(int, station_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f560364-c3d2-48bb-a4e6-669031b9b0ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Station with soil moisture data\n",
    "good_stations = list(set(station_ids) - set(station_no_variable))\n",
    "print(len(good_stations))\n",
    "print(good_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c7f2ae-c6f9-4963-b3b5-e2a3d4522438",
   "metadata": {},
   "source": [
    "### Get proportion of flagged data for each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cf820b-48c1-408e-a516-c9d75cf57ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_stations = [1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1018, 1045, 1046, 1047, 1021, 1049, 1050, 1570, 1019, 1066, 1067, 1068, 1590, 1085, 1020, 1610, 1611, 1612, 1613, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1128, 1129, 1130, 1138, 1139, 1140, 1141, 1142, 1144, 1145, 1231, 1232, 1233, 1234, 1235, 1236, 1753, 1772, 1797, 1306, 1307, 1346, 1347, 1386, 1387, 1406, 1407, 1426, 1446, 1447, 1467, 1468, 1487, 1507, 1508, 1510, 1001, 1002, 1003, 1004, 1005, 1006, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1528, 1529, 1530, 1531, 1532, 1017, 1022, 1023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b91601d-7626-4a1d-9a3f-175b2de2dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the percentage of flags in each station\n",
    "\n",
    "for i,id in enumerate(good_stations):\n",
    "    filter_condition = ('station_ID', '==', id)\n",
    "    df = pd.read_parquet('s3://uscrn-soilmoisture-ml/shiva/hydra/parquet/my_parquet.parquet',engine='pyarrow',filters=[filter_condition])\n",
    "    flags = ['AutoFlag']\n",
    "    count_dict =  {}\n",
    "    count_dict['station_id'] = id\n",
    "    for flag in flags:\n",
    "        count_dict[flag] = (1-df[flag].value_counts(normalize=True).iloc[0])\n",
    "    if i == 0:\n",
    "        flags.append('station_id')\n",
    "        count_df = pd.DataFrame([count_dict],columns = flags)\n",
    "        flags.remove('station_id')\n",
    "    else:\n",
    "        temp_df = pd.DataFrame([count_dict])\n",
    "        count_df = pd.concat([count_df, temp_df], ignore_index=True)\n",
    "        del [temp_df]\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3daed7-4f31-40a8-b62f-3466d6fa6ee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(count_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f3cd8-c17a-4343-aaea-f833ec25cec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each station and variable\n",
    "for i,id in enumerate(good_stations):\n",
    "    filter_condition = ('station_ID', '==', id)\n",
    "    df = pd.read_parquet('s3://uscrn-soilmoisture-ml/shiva/hydra/parquet/my_parquet.parquet',engine='pyarrow',filters=[filter_condition])\n",
    "    grouped_df = df.groupby(['station_ID', 'Variable'])['AutoFlag'].sum().reset_index()\n",
    "    zero_flags = grouped_df[grouped_df['AutoFlag'] == 0].reset_index(drop = True)\n",
    "    if i == 0:\n",
    "        count_df = zero_flags\n",
    "    else:\n",
    "        count_df = pd.concat([count_df, zero_flags], ignore_index=True)\n",
    "    del [grouped_df]\n",
    "    del [zero_flags]\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55bdb7e-3d94-468c-8cff-2dd99fa0c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    " station_variables_df = count_df.groupby('station_ID')['Variable'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972e0440-1e30-4461-924d-51011f60f4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stations and sensors with no flagged data\n",
    "print(station_variables_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d03b38-8788-41b5-a5a7-5d1309b84ffe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking that no other flag column is flagged\n",
    "import pyarrow.parquet as pq\n",
    "parquet_file = pq.ParquetFile('s3://uscrn-soilmoisture-ml/shiva/hydra/parquet/my_parquet.parquet')\n",
    "flags = ['AutoFlag','NoPrcpResponse','FrozenRecovery','Noise','Failure','Static','Erratic','DiurnalNoise','Toohigh','Scaling','Zero','Spike']\n",
    "for flag in flags:\n",
    "    tab = parquet_file.read(columns=[flag]).to_pandas()\n",
    "    print(\"Flag column: \", flag)\n",
    "    print(tab[flag].unique())\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ff988-1d0a-496b-9b8a-d1ee1bc9e35e",
   "metadata": {},
   "source": [
    "## Basic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86ad0ab-ac7d-49d0-91df-97820b309c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_depth(row):\n",
    "    return int(row[-4:])\n",
    "\n",
    "def any_flags(row):\n",
    "    count =  row['AutoFlag'] + row['NoPrcpResponse'] + row['FrozenRecovery'] + row['Noise'] + row['Failure'] + row['Static'] + row['Erratic'] + row['DiurnalNoise'] + row['Toohigh'] + row['Scaling'] + row['Zero'] + row['Spike']\n",
    "    if count == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ac1361-47cf-4be5-8565-55dc03db0983",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "source_parquet_file = 's3://uscrn-soilmoisture-ml/shiva/hydra/parquet/my_parquet.parquet'\n",
    "target_parquet_file = 's3://uscrn-soilmoisture-ml/shiva/hydra/parquet/preprocessed_1.parquet'\n",
    "columns = ['WBAN','station_ID','UTC_Start','Temperature','Precipitation','Tipping_Bucket','Wetness1','Wetness2','Variable','Depth','Dielectric','Volumetric','FlagBit','AutoFlag','NoPrcpResponse','FrozenRecovery','Noise','Failure','Static','Erratic','DiurnalNoise','Toohigh','Scaling','Zero','Spike']\n",
    "\n",
    "for i,id in enumerate(good_stations):\n",
    "    filter_condition = ('station_ID', '==', id)\n",
    "    df = pd.read_parquet('s3://uscrn-soilmoisture-ml/shiva/hydra/parquet/my_parquet.parquet',engine='pyarrow',filters=[filter_condition])\n",
    "    #df['Variable'].fillna(method='ffill', inplace=True)\n",
    "    df[\"Variable\"] = df[\"Variable\"].ffill()\n",
    "    df['Variable'] = df['Variable'].apply(extract_depth)\n",
    "    df['Flag'] = df.apply(lambda x: any_flags(x), axis=1)\n",
    "    df.drop(['NoPrcpResponse','FrozenRecovery','Noise','Failure','Static','Erratic','DiurnalNoise','Toohigh','Scaling','Zero','Spike'],axis=1,inplace=True)\n",
    "    if i == 0:\n",
    "        # Guess the schema of the CSV file from the first chunk\n",
    "        parquet_schema = pa.Table.from_pandas(df=df).schema\n",
    "        parquet_writer = pq.ParquetWriter(target_parquet_file, parquet_schema, compression='snappy')\n",
    "    table = pa.Table.from_pandas(df, schema=parquet_schema)\n",
    "    parquet_writer.write_table(table)\n",
    "    del [df]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5888f9ca-5bc9-41e7-b8a4-cf672f228620",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc06fd-72f3-4b7e-a724-458b3a2d1cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "colums = ['station_id','count']\n",
    "for id in station_ids:\n",
    "    dict = {}\n",
    "    dict['count'] =[]\n",
    "    dict['station_id'] = []\n",
    "    filter_condition = ('station_ID', '==', id)\n",
    "    df = pd.read_parquet('s3://uscrn-soilmoisture-ml/shiva/hydra/parquet/my_parquet.parquet',engine='pyarrow',filters=[filter_condition])\n",
    "    dict['station_id'].append(id)\n",
    "    dict['count'].append(len(df))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e06708-3216-4768-bb62-57f69339cfc0",
   "metadata": {},
   "source": [
    "### Time gap investigation\n",
    "Followed by exploration of 3 different time series preprocessing techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1044f553-888e-4319-b5d2-b1b0ce686beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_time_gaps(row):\n",
    "    filter_condition = ('station_ID', '==', row['station_ID'])\n",
    "    station = pd.read_parquet('s3://uscrn-soilmoisture-ml/shiva/hydra/parquet/my_parquet.parquet',engine='pyarrow',columns=['station_ID','UTC_Start','Variable'],filters=[filter_condition])\n",
    "    #station = df[df['station_ID'] == row['station_ID']].copy()\n",
    "    no_gap, one_hr_gap, two_hr_gap, five_hr_gap = [], [], [], []\n",
    "\n",
    "    for var in row['Variable'][:-1]:\n",
    "        sensor = station[station['Variable'] == var]\n",
    "        sensor_sorted = sensor.sort_values('UTC_Start')\n",
    "        diff = sensor_sorted['UTC_Start'].diff()\n",
    "        one_hr_gap_exists = any(diff > pd.to_timedelta('1 hour'))\n",
    "        two_hr_gap_exists = any(diff > pd.to_timedelta('2 hours'))\n",
    "        five_hr_gap_exists = any(diff > pd.to_timedelta('5 hours'))\n",
    "\n",
    "\n",
    "        if not one_hr_gap_exists:\n",
    "            no_gap.append(var)\n",
    "        elif five_hr_gap_exists:\n",
    "            five_hr_gap.append(var)\n",
    "        elif two_hr_gap_exists:\n",
    "            two_hr_gap.append(var)\n",
    "        else:\n",
    "            one_hr_gap.append(var)\n",
    "    del [station]\n",
    "    return pd.Series([no_gap, one_hr_gap, two_hr_gap, five_hr_gap])\n",
    "\n",
    "result_df = station_variables_df.apply(lambda row: check_time_gaps(row), axis=1, result_type='expand')\n",
    "result_df.columns = ['NoGap', '1HrGap', '2HrGap', '5HrGap+']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8415e28a-37f4-4e0c-b261-44b25ee286dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_gap_df = station_variables_df.join(result_df)\n",
    "time_gap_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37386a3-cb2c-4e8c-b272-b9860bd3831b",
   "metadata": {},
   "source": [
    "### Differencing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3046e8-2e57-4d31-94b7-97dab62557b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "from bokeh.palettes import HighContrast3\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.layouts import row, column\n",
    "from bokeh.models import ColumnDataSource, RangeTool\n",
    "from bokeh.transform import factor_cmap, factor_mark\n",
    "\n",
    "def plot_differenced_series(station, sensor,col,df,diff=0):\n",
    "    #Replotting with Bokeh\n",
    "    s1 = figure(title=f'Station {station} sensor {sensor} original data',width=1250, height=450, background_fill_color=\"#fafafa\",x_axis_type=\"datetime\")\n",
    "    Flag = ['0','1']\n",
    "    df.Flag = df.Flag.astype('str')\n",
    "    s1.scatter(\"UTC_Start\", col, source=df ,size=4, alpha=0.5, marker=\"circle\",color=factor_cmap(\"Flag\", 'Category10_3', Flag))\n",
    "    \n",
    "    df['diffrenced_series'] = df[col].diff(periods=diff)\n",
    "    print(df.columns)\n",
    "    s2 = figure(title=f'Station {station} sensor {sensor} Differenced {diff} order',width=1250, height=450, background_fill_color=\"#fafafa\",x_axis_type=\"datetime\")\n",
    "    s2.scatter(\"UTC_Start\",\"diffrenced_series\",source=df, size=4, alpha=0.5,marker=\"circle\",color=factor_cmap(\"Flag\", 'Category10_3', Flag))\n",
    "    print('after 2 obj')\n",
    "    # put the results in a row and show\n",
    "    show(column(s1, s2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda4c9b7-686a-41cb-a4a4-587ca5723f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_parquet_file = 's3://uscrn-soilmoisture-ml/shiva/hydra/parquet/preprocessed_1.parquet'\n",
    "stations = [1017, 1023,1528]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76435106-bc13-4367-85e5-14f923ed8665",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for station in stations:\n",
    "    filter_condition = ('station_ID', '==', station)\n",
    "    df = pd.read_parquet(target_parquet_file, engine='pyarrow',filters=[filter_condition])\n",
    "    print(station)\n",
    "    variables = df['Variable'].unique()\n",
    "    print(variables)\n",
    "    for sensor in variables:\n",
    "        plot_differenced_series(station,sensor,'Volumetric',df[df.Variable == sensor],1)\n",
    "        break\n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219fec18-bcaf-41ba-8abd-7e3fc2fdc189",
   "metadata": {},
   "source": [
    "### Fourier Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10315a85-4744-4119-9997-bca43d4d2125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.palettes import HighContrast3\n",
    "from bokeh.plotting import figure, show\n",
    "def fourier_transform(station, sensor):\n",
    "    filter_condition = [('station_ID', '==', station),('Variable','==',sensor)]\n",
    "    target_parquet_file = 's3://uscrn-soilmoisture-ml/shiva/hydra/parquet/preprocessed_1.parquet'\n",
    "    sensor_df = pd.read_parquet(target_parquet_file, engine='pyarrow',filters=filter_condition)\n",
    "    sensor_df.fillna(0,inplace=True)\n",
    "    #print(sensor_df.head())\n",
    "    ft = np.fft.fft(sensor_df['Volumetric'])\n",
    "    frequencies = np.fft.fftfreq(len(sensor_df['Volumetric']), d=0.1)\n",
    "    print('ft: ',len(ft))\n",
    "    print('freq: ', len(frequencies))\n",
    "    plot = figure(title=f\"Frequency Domain for Station {station} and Sensor {sensor}\")\n",
    "    plot.line(frequencies, np.abs(ft))#, size=10, marker=\"circle\")\n",
    "    show(plot)\n",
    "    #plt.plot(frequencies, np.abs(ft))\n",
    "    #plt.title(f\"Frequency Domain for Station {station} and Sensor {sensor}\")\n",
    "    #plt.ylim([-1, 10])\n",
    "    #plt.xlim([0, 5000])\n",
    "    #plt.xlabel(\"Frequency\")\n",
    "    #plt.ylabel(\"Amplitude\")\n",
    "    #plt.show()\n",
    "\n",
    "    return ft, frequencies, sensor_df['UTC_Start'], sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee366ef-a42a-44c8-872a-c6ec33647aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_freq(threshold, ft, freqs, dates, sensor):\n",
    "    threshold_frequency = threshold\n",
    "    ft[np.abs(freqs) == threshold_frequency] = 0\n",
    "    cleaned_signal = np.fft.ifft(ft)\n",
    "    plt.figure(figsize=(10, 16))\n",
    "    plot = figure(title=f\"Sensor {sensor} (After Denoising), Threshold {threshold}\",x_axis_type=\"datetime\")\n",
    "    plot.scatter(dates, np.real(cleaned_signal), size=2, marker=\"circle\", alpha=0.4)\n",
    "    show(plot)\n",
    "    \n",
    "    #sns.scatterplot(x=dates, y=np.real(cleaned_signal), s=5, alpha=0.5)\n",
    "    #plt.title(f\"Sensor {sensor} (After Denoising), Threshold {threshold}\")\n",
    "    #plt.xlabel(\"Time\")\n",
    "    #plt.ylabel(\"Volumetric Amplitude\")\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ef0ab0-c03b-45f1-aa8e-d92494397e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Station 1017, sensor 1005 \n",
    "ft, frequencies, dates, sensor = fourier_transform(1017, 1005)\n",
    "remove_freq(1, ft, frequencies, dates, sensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaba94ff-39c3-4f5e-8898-6549acfce19f",
   "metadata": {},
   "source": [
    "### Rolling averages/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc19f05c-415f-4e2f-859b-394ff79a6de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_calculations(df):\n",
    "    df = df.sort_values(by=['UTC_Start'])\n",
    "    df['RollingAvg'] = df['Volumetric'].rolling(window_size, min_periods=window_size, center = True).mean()\n",
    "    df['RollingStd'] = df['Volumetric'].rolling(window_size, min_periods=window_size, center = True).std()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3562ca47-c050-4d0b-a3d6-e19c4a8a2233",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=12\n",
    "station = 1017\n",
    "\n",
    "filter_condition = [('station_ID', '==', station)]\n",
    "target_parquet_file = 's3://uscrn-soilmoisture-ml/shiva/hydra/parquet/preprocessed_1.parquet'\n",
    "sensor_df = pd.read_parquet(target_parquet_file, engine='pyarrow',filters=filter_condition)\n",
    "#windows = [13,25,49]\n",
    "result_df = sensor_df.groupby(['station_ID', 'Variable']).apply(rolling_calculations)\n",
    "result_df = result_df.droplevel([0,1])\n",
    "sensor_df[f\"StVol_{window_size}hrs\"] = (result_df['Volumetric'] - result_df['RollingAvg']) / result_df['RollingStd']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x = 'UTC_Start', y = f\"StVol_{window_size}hrs\", data = sensor_df, s = 5, alpha=0.5)\n",
    "plt.title(f'{station} {sensor} {window_size}', fontsize = 30)\n",
    "plt.xlabel('Date', fontsize = 20)\n",
    "plt.xticks(rotation = 30)\n",
    "plt.ylabel(f\"StVol_{window_size}hrs\", fontsize = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df616ba8-a3e6-4f3c-8902-70a012778d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "from bokeh.layouts import row, column\n",
    "from bokeh.models import ColumnDataSource, RangeTool\n",
    "from bokeh.transform import factor_cmap, factor_mark\n",
    "\n",
    "def plot_rolling_avg(station, sensor, window_size):\n",
    "    filter_condition = [('station_ID', '==', station),('Variable','==',sensor)]\n",
    "    target_parquet_file = 's3://uscrn-soilmoisture-ml/shiva/hydra/parquet/preprocessed_1.parquet'\n",
    "    sensor_df = pd.read_parquet(target_parquet_file, engine='pyarrow',filters=filter_condition)\n",
    "    \n",
    "    #windows = [13,25,49]\n",
    "    result_df = sensor_df.groupby(['station_ID', 'Variable']).apply(rolling_calculations)\n",
    "    result_df = result_df.droplevel([0,1])\n",
    "    sensor_df[f\"StVol_{window_size}hrs\"] = (result_df['Volumetric'] - result_df['RollingAvg']) / result_df['RollingStd']\n",
    "    Flag = ['0','1']\n",
    "    sensor_df.Flag = sensor_df.Flag.astype('str')\n",
    "    #Replotting with Bokeh\n",
    "    s1 = figure(title=f'Station: {station} Sensor: {sensor} window_size: {window_size}',width=1250, height=450, background_fill_color=\"#fafafa\",x_axis_type=\"datetime\",x_range=(sensor_df.UTC_Start[1500], sensor_df.UTC_Start[2500]))\n",
    "    s1.scatter(\"UTC_Start\", f\"StVol_{window_size}hrs\", source=sensor_df,size=4, alpha=0.5, marker=\"circle\",color=factor_cmap(\"Flag\", 'Category10_3', Flag))\n",
    "\n",
    "    select = figure(title=\"Drag the middle and edges of the selection box to change the range above\",\n",
    "                height=130, width=800, y_range=s1.y_range,\n",
    "                x_axis_type=\"datetime\", y_axis_type=None,\n",
    "                tools=\"\", toolbar_location=None, background_fill_color=\"#efefef\")\n",
    "\n",
    "    range_tool = RangeTool(x_range=s1.x_range)\n",
    "    range_tool.overlay.fill_color = \"navy\"\n",
    "    range_tool.overlay.fill_alpha = 0.2\n",
    "    \n",
    "    select.scatter(\"UTC_Start\", f\"StVol_{window_size}hrs\",source=sensor_df,color=factor_cmap(\"Flag\", 'Category10_3', Flag))\n",
    "    select.ygrid.grid_line_color = None\n",
    "    select.add_tools(range_tool)\n",
    "\n",
    "    # put the results in a row and show\n",
    "    show(column(s1,select))\n",
    "    '''\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x = 'UTC_Start', y = f\"StVol_{window_size}hrs\", data = sensor_df, s = 5, alpha=0.5)\n",
    "    plt.title(f'{station} {sensor} {window_size}', fontsize = 30)\n",
    "    plt.xlabel('Date', fontsize = 20)\n",
    "    plt.xticks(rotation = 30)\n",
    "    plt.ylabel(f\"StVol_{window_size}hrs\", fontsize = 20)\n",
    "    '''\n",
    "window_size=72\n",
    "plot_rolling_avg(1017,1005,window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e337ed-115c-407f-b63c-0eefe2456185",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=168\n",
    "plot_rolling_avg(1017,1005,window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f989370-b569-470a-b6fa-d7c27edce01c",
   "metadata": {},
   "source": [
    "### Climatology preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5378f8-306f-48d4-854a-69ac012d2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For a given month-day-hour, take all points across all years and +- 2 days (window), calculate the mean/std and use this to standardize that point\n",
    "target_parquet_file = 's3://uscrn-soilmoisture-ml/shiva/hydra/parquet/preprocessed_1.parquet'\n",
    "stations = [1017, 1023,1528]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb861db-0e62-4d33-b7f6-83bc860ae985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "filter_condition = ('station_ID', '==', 1017)\n",
    "df = pd.read_parquet(target_parquet_file, engine='pyarrow',filters=[filter_condition])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857236e8-b3d1-4d65-a783-e249c475f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def preprocessing(station,sensor, column, df, window):\n",
    "    df_1 = df[df.Variable==sensor]\n",
    "    df_1.loc[:,['month']] = df_1['UTC_Start'].dt.month\n",
    "    df_1.loc[:,['day']] = df_1['UTC_Start'].dt.day\n",
    "    df_1.loc[:,['hour']] = df_1['UTC_Start'].dt.hour\n",
    "    climatology_dict = {}\n",
    "    window = window\n",
    "    td = timedelta(days=window)\n",
    "    start_date = datetime.strptime('01-01-00','%m-%d-%H')\n",
    "    end_date = datetime.strptime('12-31-23','%m-%d-%H')\n",
    "    print(start_date.strftime('%m-%d-%H'))\n",
    "    it = timedelta(hours=1)\n",
    "    while start_date <= end_date:\n",
    "        window_date_start = start_date - td\n",
    "        temp_df = pd.Series()\n",
    "        while window_date_start <= start_date + td:\n",
    "            #print(df[(df.month == window_date_start.month) & (df.day == window_date_start.day) & (df.hour == window_date_start.hour)].Volumetric)\n",
    "            temp_df = pd.concat([df_1[(df_1.month == window_date_start.month) & (df_1.day == window_date_start.day) & (df_1.hour == window_date_start.hour)][column], temp_df], ignore_index=True)\n",
    "            window_date_start = window_date_start + td\n",
    "        climatology_dict[start_date.strftime('%m-%d-%H')] = (temp_df.mean(), temp_df.std(), temp_df.isna().mean())\n",
    "        start_date = start_date + it\n",
    "        del [temp_df]\n",
    "\n",
    "    #For Leap year\n",
    "    start_date = datetime.strptime('2012-02-29-00','%Y-%m-%d-%H')\n",
    "    end_date = datetime.strptime('2012-02-29-23','%Y-%m-%d-%H')\n",
    "    it = timedelta(hours=1)\n",
    "    while start_date <= end_date:\n",
    "        window_date_start = start_date - td\n",
    "        temp_df = pd.Series()\n",
    "        while window_date_start <= start_date + td:\n",
    "            #print(df[(df.month == window_date_start.month) & (df.day == window_date_start.day) & (df.hour == window_date_start.hour)].Volumetric)\n",
    "            temp_df = pd.concat([df_1[(df_1.month == window_date_start.month) & (df_1.day == window_date_start.day) & (df_1.hour == window_date_start.hour)]['Volumetric'], temp_df], ignore_index=True)\n",
    "            window_date_start = window_date_start + td\n",
    "        climatology_dict[start_date.strftime('%m-%d-%H')] = (temp_df.mean(), temp_df.std(), temp_df.isna().mean())\n",
    "        start_date = start_date + it\n",
    "        del [temp_df]\n",
    "    return climatology_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5a1c3c-2504-498f-b99e-a516e33f16ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.palettes import HighContrast3\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.layouts import row, column\n",
    "from bokeh.models import ColumnDataSource, RangeTool\n",
    "from bokeh.transform import factor_cmap, factor_mark\n",
    "\n",
    "def plot_preprocessed(df, station, sensor):\n",
    "\n",
    "    #palette = d3['Category10'][len(df['Flag'].unique())]\n",
    "    #color_map = bmo.CategoricalColorMapper(factors=df['Flag'].unique(),palette=palette)\n",
    "    #Replotting with Bokeh\n",
    "    #source = ColumnDataSource.from_df(df)\n",
    "    Flag = ['0','1']\n",
    "    df.Flag = df.Flag.astype('str')\n",
    "    s1 = figure(title=f'Station {station} sensor {sensor}',width=1250, height=450, background_fill_color=\"#fafafa\",x_axis_type=\"datetime\")\n",
    "    s1.scatter('UTC_Start', 'Volumetric',  source = df, size=4, alpha=0.5, marker=\"circle\", legend_field='Flag',color=factor_cmap(\"Flag\", 'Category10_3', Flag))\n",
    "    \n",
    "    s2 = figure(title=f'Station {station} sensor {sensor} preprocessed',width=1250, height=450, background_fill_color=\"#fafafa\",x_axis_type=\"datetime\")\n",
    "    s2.scatter('UTC_Start','preprocessed_volumetric',source = df, size=4, alpha=0.5,marker=\"circle\", legend_field='Flag',color=factor_cmap(\"Flag\", 'Category10_3', Flag))\n",
    "    print('after 2 obj')\n",
    "    # put the results in a row and show\n",
    "    show(column(s1, s2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb1227e-d794-44d8-9861-35d4baf456fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(row):\n",
    "    return (row['Volumetric'] - climatology_dict[row['preprocess_key']][0])/climatology_dict[row['preprocess_key']][1]\n",
    "\n",
    "for station in stations:\n",
    "    filter_condition = ('station_ID', '==', station)\n",
    "    df = pd.read_parquet(target_parquet_file, engine='pyarrow',filters=[filter_condition])\n",
    "    variables = df['Variable'].unique()\n",
    "    print(variables)\n",
    "    for sensor in variables:\n",
    "        df_1 = df[df.Variable==sensor]\n",
    "        climatology_dict = preprocessing(station,sensor,'Volumetric',df, 1)\n",
    "        df_1.loc[:,['preprocess_key']] = df_1.UTC_Start.dt.strftime('%m-%d-%H')\n",
    "        df_1.loc[:,['preprocessed_volumetric']] = df_1.apply(lambda row: func(row), axis=1)\n",
    "        plot_preprocessed(df_1, station, sensor)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e8c513-e7d0-4dfc-90f8-328736fefa1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "climatology_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
